{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d74e62",
   "metadata": {},
   "source": [
    "# Data Cleaning — Goodbooks-10k Top 100 (Updated)\n",
    "**Objetivo:**  \n",
    " \n",
    "1. Cargar y explorar los ficheros de ratings, metadatos y tags.  \n",
    "2. Seleccionar los 100 libros más valorados.  \n",
    "3. Filtrar usuarios con actividad mínima (≥ 20 ratings).  \n",
    "4. Binarizar las calificaciones (≥ 4 → “like”).  \n",
    "5. Pivotar a matriz dispersa 0/1 `R_binary`.  \n",
    "6. Extraer y guardar metadata enriquecida (título, autor, top-tags).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f60b382",
   "metadata": {},
   "source": [
    "Importamos las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eef99bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e47586",
   "metadata": {},
   "source": [
    "Cargamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "426d974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('./data/ratings.csv')\n",
    "books = pd.read_csv('./data/books.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b99fb3a",
   "metadata": {},
   "source": [
    "Limpieza: eliminamos usuarios y libros con menos de 5 valoraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d442d417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usuarios con ≥5 ratings: 53424\n",
      "Libros   con ≥5 ratings: 10000\n"
     ]
    }
   ],
   "source": [
    "min_ratings = 5\n",
    "user_counts = ratings['user_id'].value_counts()\n",
    "book_counts = ratings['book_id'].value_counts()\n",
    "\n",
    "ratings_clean = ratings[\n",
    "    ratings['user_id'].isin(user_counts[user_counts >= min_ratings].index) &\n",
    "    ratings['book_id'].isin(book_counts[book_counts >= min_ratings].index)\n",
    "].copy()\n",
    "\n",
    "n_users_clean = ratings_clean['user_id'].nunique()\n",
    "n_items_clean = ratings_clean['book_id'].nunique()\n",
    "\n",
    "print(f\"Usuarios con ≥{min_ratings} ratings: {n_users_clean}\")\n",
    "print(f\"Libros   con ≥{min_ratings} ratings: {n_items_clean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ec3225",
   "metadata": {},
   "source": [
    "Dividimos en train y test, tomando valoración y usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "897c7f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(ratings_clean, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed3ff84",
   "metadata": {},
   "source": [
    "Baseline: media global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1814e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_mean = train['rating'].mean()\n",
    "test['pred_global_mean'] = global_mean\n",
    "\n",
    "# mean squared error y mean absolute error\n",
    "rmse_global = np.sqrt(mean_squared_error(test['rating'], test['pred_global_mean']))\n",
    "mae_global = mean_absolute_error(test['rating'], test['pred_global_mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8a19ba",
   "metadata": {},
   "source": [
    "Baseline: media por usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62fc67cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mean = train.groupby('user_id')['rating'].mean()\n",
    "test = test.join(user_mean, on='user_id', rsuffix='_user_mean')\n",
    "test['pred_user_mean'] = test['rating_user_mean'].fillna(global_mean)\n",
    "\n",
    "rmse_user = np.sqrt(mean_squared_error(test['rating'], test['pred_user_mean']))\n",
    "mae_user = mean_absolute_error(test['rating'], test['pred_user_mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecf482f",
   "metadata": {},
   "source": [
    "Mostramos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b390784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de Baseline\n",
      "       Baseline      RMSE       MAE\n",
      "0  Global Mean  0.990329  0.774018\n",
      "1    User Mean  0.893159  0.700713\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Baseline': ['Global Mean', 'User Mean'],\n",
    "    'RMSE': [rmse_global, rmse_user],\n",
    "    'MAE': [mae_global, mae_user]\n",
    "})\n",
    "\n",
    "print('Resultados de Baseline\\n', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8255b5e",
   "metadata": {},
   "source": [
    "RMSE (~1): en promedio, la predicción se equivoca en 1 estrella sobre 5.\n",
    "MAE (~0.7): el error medio absoluto es  de, más o menos, 0.7 estrellas.\n",
    "\n",
    "Al pasar de global mean a user mean el RMSE se reduce a 0.89, por lo que ya tenemos una mejora del 10%. Estos datos serán el baseline contra el que compararemos los 3 métodos de filtrado colaborativo para ver si nos son útiles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38461afe",
   "metadata": {},
   "source": [
    "Para comenzar con las implementaciones guardaremos en nuestra carpeta data/processed ratings_clean, train y test para mayor orden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c8e3ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos guardados en 'data/processed':\n",
      "['ratings_clean.csv', 'test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "processed_dir = 'data/processed'\n",
    "\n",
    "if not os.path.isdir(processed_dir):\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "    \n",
    "ratings_clean.to_csv(os.path.join(processed_dir, 'ratings_clean.csv'), index=False)\n",
    "train.to_csv(os.path.join(processed_dir, 'train.csv'), index=False)\n",
    "test.to_csv(os.path.join(processed_dir, 'test.csv'), index=False)\n",
    "\n",
    "print(\"Datos guardados en 'data/processed':\")\n",
    "print(os.listdir(processed_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bec445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_ndcg(u, pred_matrix, test_df, K=5, rel_col='rel'):\n",
    "    \"\"\"\n",
    "    nDCG@K para el usuario u (0-based).\n",
    "    test_df debe tener columnas ['user_id','book_id', rel_col].\n",
    "    rel_col es la relevancia (binaria o graduada).\n",
    "    \"\"\"\n",
    "    user_ratings = test_df[test_df.user_id == (u+1)]\n",
    "    if user_ratings.empty:\n",
    "        return None\n",
    "\n",
    "    true_rels = {\n",
    "        int(row.book_id)-1: row[rel_col]\n",
    "        for _, row in user_ratings.iterrows()\n",
    "    }\n",
    "\n",
    "    scores = pred_matrix[u]\n",
    "    # Top-K \n",
    "    top_k = np.argsort(scores)[::-1][:K]\n",
    "\n",
    "    dcg = 0.0\n",
    "    for rank, item in enumerate(top_k, start=1):\n",
    "        rel = true_rels.get(item, 0)\n",
    "        dcg += (2**rel - 1) / np.log2(rank + 1)\n",
    "\n",
    "    ideal_rels = sorted(true_rels.values(), reverse=True)[:K]\n",
    "    idcg = sum((2**rel - 1) / np.log2(idx + 1)\n",
    "               for idx, rel in enumerate(ideal_rels, start=1))\n",
    "\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def get_ndcg(pred_matrix, test_df, K=5, rel_col='rel'):\n",
    "    \"\"\"\n",
    "    nDCG@K promedio sobre todos los usuarios con al menos una valoración.\n",
    "    \"\"\"\n",
    "    total, count = 0.0, 0\n",
    "    num_users = pred_matrix.shape[0]\n",
    "    for u in range(num_users):\n",
    "        val = get_user_ndcg(u, pred_matrix, test_df, K, rel_col)\n",
    "        if val is not None:\n",
    "            total += val\n",
    "            count += 1\n",
    "    return (total / count) if count > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feb230f",
   "metadata": {},
   "source": [
    "#### Baseline: User-Mean Predictor\n",
    "\n",
    "1. **¿Qué hace?** Para cada cada usuario siempre su **media histórica** de ratings (o la media global si es nuevo).  \n",
    "2. **Ventaja**: captura el sesgo individual de cada lector (alguien exigente vs. alguien generoso).  \n",
    "3. **Regresión**: evalúa MAE/RMSE comparando la media de usuario vs. rating real.  \n",
    "4. **Clasificación**: binariza con umbral = floor(media global) para medir Precision, Recall y F1.  \n",
    "5. **Ranking**: usa la misma matriz de predicciones para calcular nDCG@10, mostrando su capacidad de ordenar ítems.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70b0ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import floor\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "train_df = pd.read_csv('./data/processed/train.csv')\n",
    "test_df  = pd.read_csv('./data/processed/test.csv')\n",
    "\n",
    "# medias\n",
    "global_mean = train_df.rating.mean()\n",
    "item_mean   = train_df.groupby('book_id').rating.mean().to_dict()\n",
    "\n",
    "# item-mean como predicción\n",
    "NUM_USERS = int(max(train_df.user_id.max(), test_df.user_id.max()))\n",
    "NUM_ITEMS = int(max(train_df.book_id.max(), test_df.book_id.max()))\n",
    "pred_matrix = np.zeros((NUM_USERS, NUM_ITEMS), dtype=np.float32)\n",
    "\n",
    "for uid in range(1, NUM_USERS+1):\n",
    "    for iid in range(1, NUM_ITEMS+1):\n",
    "        mu_i = item_mean.get(iid, global_mean)\n",
    "        pred_matrix[uid-1, iid-1] = mu_i\n",
    "\n",
    "# regresión en test\n",
    "y_true, y_pred = [], []\n",
    "for _, r in test_df.iterrows():\n",
    "    u, i = int(r.user_id)-1, int(r.book_id)-1\n",
    "    y_true.append(r.rating)\n",
    "    y_pred.append(pred_matrix[u, i])\n",
    "\n",
    "mae  = mean_absolute_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# binarizamos y métricas de clasificación\n",
    "thr = floor(global_mean)\n",
    "y_true_bin = (np.array(y_true) >= thr).astype(int)\n",
    "y_pred_bin = (np.array(y_pred) >= thr).astype(int)\n",
    "\n",
    "prec = precision_score(y_true_bin, y_pred_bin, zero_division=0)\n",
    "rec  = recall_score   (y_true_bin, y_pred_bin, zero_division=0)\n",
    "f1   = f1_score       (y_true_bin, y_pred_bin, zero_division=0)\n",
    "\n",
    "# ranking uniforme\n",
    "ndcg10 = 0.0\n",
    "\n",
    "# imprimimos\n",
    "print(f\"Item-Mean Baseline → MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "print(f\"Item-Mean Baseline → Precision: {prec:.3f}, Recall: {rec:.3f}, F1: {f1:.3f}\")\n",
    "print(f\"Item-Mean Baseline → nDCG@10: {ndcg10:.3f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
