{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d74e62",
   "metadata": {},
   "source": [
    "# Data Cleaning — Goodbooks-10k Top 100 (Updated)\n",
    "**Objetivo:**  \n",
    " \n",
    "1. Cargar y explorar los ficheros de ratings, metadatos y tags.  \n",
    "2. Seleccionar los 100 libros más valorados.  \n",
    "3. Filtrar usuarios con actividad mínima (≥ 20 ratings).  \n",
    "4. Binarizar las calificaciones (≥ 4 → “like”).  \n",
    "5. Pivotar a matriz dispersa 0/1 `R_binary`.  \n",
    "6. Extraer y guardar metadata enriquecida (título, autor, top-tags).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f60b382",
   "metadata": {},
   "source": [
    "Importamos las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eef99bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e47586",
   "metadata": {},
   "source": [
    "Cargamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "426d974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('./data/ratings.csv')\n",
    "books = pd.read_csv('./data/books.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b99fb3a",
   "metadata": {},
   "source": [
    "Limpieza: eliminamos usuarios y libros con menos de 5 valoraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d442d417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usuarios con ≥5 ratings: 53424\n",
      "Libros   con ≥5 ratings: 10000\n"
     ]
    }
   ],
   "source": [
    "min_ratings = 5\n",
    "user_counts = ratings['user_id'].value_counts()\n",
    "book_counts = ratings['book_id'].value_counts()\n",
    "\n",
    "ratings_clean = ratings[\n",
    "    ratings['user_id'].isin(user_counts[user_counts >= min_ratings].index) &\n",
    "    ratings['book_id'].isin(book_counts[book_counts >= min_ratings].index)\n",
    "].copy()\n",
    "\n",
    "n_users_clean = ratings_clean['user_id'].nunique()\n",
    "n_items_clean = ratings_clean['book_id'].nunique()\n",
    "\n",
    "print(f\"Usuarios con ≥{min_ratings} ratings: {n_users_clean}\")\n",
    "print(f\"Libros   con ≥{min_ratings} ratings: {n_items_clean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ec3225",
   "metadata": {},
   "source": [
    "Dividimos en train y test, tomando valoración y usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "897c7f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(ratings_clean, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed3ff84",
   "metadata": {},
   "source": [
    "Baseline: media global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1814e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_mean = train['rating'].mean()\n",
    "test['pred_global_mean'] = global_mean\n",
    "\n",
    "# mean squared error y mean absolute error\n",
    "rmse_global = np.sqrt(mean_squared_error(test['rating'], test['pred_global_mean']))\n",
    "mae_global = mean_absolute_error(test['rating'], test['pred_global_mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8a19ba",
   "metadata": {},
   "source": [
    "Baseline: media por usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62fc67cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mean = train.groupby('user_id')['rating'].mean()\n",
    "test = test.join(user_mean, on='user_id', rsuffix='_user_mean')\n",
    "test['pred_user_mean'] = test['rating_user_mean'].fillna(global_mean)\n",
    "\n",
    "rmse_user = np.sqrt(mean_squared_error(test['rating'], test['pred_user_mean']))\n",
    "mae_user = mean_absolute_error(test['rating'], test['pred_user_mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecf482f",
   "metadata": {},
   "source": [
    "Mostramos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b390784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de Baseline\n",
      "       Baseline      RMSE       MAE\n",
      "0  Global Mean  0.990329  0.774018\n",
      "1    User Mean  0.893159  0.700713\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Baseline': ['Global Mean', 'User Mean'],\n",
    "    'RMSE': [rmse_global, rmse_user],\n",
    "    'MAE': [mae_global, mae_user]\n",
    "})\n",
    "\n",
    "print('Resultados de Baseline\\n', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8255b5e",
   "metadata": {},
   "source": [
    "RMSE (~1): en promedio, la predicción se equivoca en 1 estrella sobre 5.\n",
    "MAE (~0.7): el error medio absoluto es  de, más o menos, 0.7 estrellas.\n",
    "\n",
    "Al pasar de global mean a user mean el RMSE se reduce a 0.89, por lo que ya tenemos una mejora del 10%. Estos datos serán el baseline contra el que compararemos los 3 métodos de filtrado colaborativo para ver si nos son útiles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38461afe",
   "metadata": {},
   "source": [
    "Para comenzar con las implementaciones guardaremos en nuestra carpeta data/processed ratings_clean, train y test para mayor orden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c8e3ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos guardados en 'data/processed':\n",
      "['ratings_clean.csv', 'test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "processed_dir = 'data/processed'\n",
    "\n",
    "if not os.path.isdir(processed_dir):\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "    \n",
    "ratings_clean.to_csv(os.path.join(processed_dir, 'ratings_clean.csv'), index=False)\n",
    "train.to_csv(os.path.join(processed_dir, 'train.csv'), index=False)\n",
    "test.to_csv(os.path.join(processed_dir, 'test.csv'), index=False)\n",
    "\n",
    "print(\"Datos guardados en 'data/processed':\")\n",
    "print(os.listdir(processed_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70b0ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# 1) Aseguramos que test_df tiene columna de relevancia binaria\n",
    "test_df['rel'] = (test_df.rating >= 4).astype(int)\n",
    "\n",
    "# 2) Verdaderos\n",
    "y_true    = test_df.rating.values\n",
    "y_true_bin= test_df.rel.values\n",
    "\n",
    "# 3) Predicciones baseline\n",
    "global_mean = train_df.rating.mean()\n",
    "user_mean   = train_df.groupby('user_id').rating.mean()\n",
    "\n",
    "# arrays de predicción\n",
    "y_pred_global = np.full_like(y_true, fill_value=global_mean, dtype=float)\n",
    "y_pred_user   = [\n",
    "    user_mean.loc[row.user_id] if row.user_id in user_mean.index else global_mean\n",
    "    for _, row in test_df.iterrows()\n",
    "]\n",
    "\n",
    "# 4) Regresión\n",
    "mae_global, rmse_global = mean_absolute_error(y_true, y_pred_global), \\\n",
    "                          np.sqrt(mean_squared_error(y_true, y_pred_global))\n",
    "mae_user,   rmse_user   = mean_absolute_error(y_true, y_pred_user), \\\n",
    "                          np.sqrt(mean_squared_error(y_true, y_pred_user))\n",
    "\n",
    "# 5) Clasificación binaria (umbral 4)\n",
    "y_pred_global_bin = (np.array(y_pred_global) >= 4).astype(int)\n",
    "y_pred_user_bin   = (np.array(y_pred_user)   >= 4).astype(int)\n",
    "\n",
    "prec_g = precision_score(y_true_bin, y_pred_global_bin, zero_division=0)\n",
    "rec_g  = recall_score   (y_true_bin, y_pred_global_bin, zero_division=0)\n",
    "f1_g   = f1_score       (y_true_bin, y_pred_global_bin, zero_division=0)\n",
    "\n",
    "prec_u = precision_score(y_true_bin, y_pred_user_bin, zero_division=0)\n",
    "rec_u  = recall_score   (y_true_bin, y_pred_user_bin, zero_division=0)\n",
    "f1_u   = f1_score       (y_true_bin, y_pred_user_bin, zero_division=0)\n",
    "\n",
    "# 6) nDCG@5\n",
    "# construimos matrices completas de predicción para ranking\n",
    "pred_global_mat = np.full((NUM_USERS, NUM_ITEMS), global_mean)\n",
    "pred_user_mat   = np.zeros((NUM_USERS, NUM_ITEMS))\n",
    "for _, row in test_df.iterrows():\n",
    "    u = int(row.user_id)-1\n",
    "    i = int(row.book_id)-1\n",
    "    pred_user_mat[u,i] = user_mean.loc[row.user_id]\n",
    "\n",
    "ndcg_g = get_ndcg(pred_global_mat, test_df, K=5, rel_col='rel')\n",
    "ndcg_u = get_ndcg(pred_user_mat,   test_df, K=5, rel_col='rel')\n",
    "\n",
    "# 7) Mostramos en DataFrame\n",
    "results = pd.DataFrame({\n",
    "    'Baseline': ['Global Mean', 'User Mean'],\n",
    "    'RMSE':     [rmse_global, rmse_user],\n",
    "    'MAE':      [mae_global, mae_user],\n",
    "    'Precision':[prec_g, prec_u],\n",
    "    'Recall':   [rec_g,  rec_u],\n",
    "    'F1':       [f1_g,   f1_u],\n",
    "    'nDCG@5':   [ndcg_g, ndcg_u]\n",
    "})\n",
    "\n",
    "print('Resultados de Baseline:\\n', results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
