{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5078a3d3",
   "metadata": {},
   "source": [
    "### 1. Importamos librerías y cargamos los datos procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "805b96cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "processed_dir = 'data/processed'\n",
    "train_df = pd.read_csv(os.path.join(processed_dir, 'train.csv'))\n",
    "test_df  = pd.read_csv(os.path.join(processed_dir, 'test.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5523e",
   "metadata": {},
   "source": [
    "### 2. Declaramos nuestros hiperparámetros y constantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24650b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITERATIONS = 10      # iteraciones\n",
    "MIN_RATING, MAX_RATING = 1.0, 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e15281",
   "metadata": {},
   "source": [
    "#### Matriz de valoraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf6d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista de listas (0 donde no hay rating)\n",
    "num_users = int(max(train_df.user_id.max(), test_df.user_id.max()))\n",
    "num_items = int(max(train_df.book_id.max(), test_df.book_id.max()))\n",
    "R = [[0.0]*num_items for _ in range(num_users)]\n",
    "for _, row in train_df.iterrows():\n",
    "    u = int(row.user_id) - 1\n",
    "    i = int(row.book_id) - 1\n",
    "    R[u][i] = float(row.rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29086b",
   "metadata": {},
   "source": [
    "### 3. Inicializamos el modelo, hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00c529e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FACTORS    = 7       # f\n",
    "LEARNING_RATE  = 0.001   # γ\n",
    "REGULARIZATION = 0.1     # λ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3767dc5",
   "metadata": {},
   "source": [
    "### 4. Creamos P y Q con uniformes en [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d6847f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "P = [[random.random() for _ in range(NUM_FACTORS)] for _ in range(num_users)]\n",
    "Q = [[random.random() for _ in range(NUM_FACTORS)] for _ in range(num_items)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8079dd9d",
   "metadata": {},
   "source": [
    "### 5. SGD iterativo \n",
    "(30 mins aproximadamente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef9d328f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración 1/10\n",
      "Iteración 2/10\n",
      "Iteración 3/10\n",
      "Iteración 4/10\n",
      "Iteración 5/10\n",
      "Iteración 6/10\n",
      "Iteración 7/10\n",
      "Iteración 8/10\n",
      "Iteración 9/10\n",
      "Iteración 10/10\n"
     ]
    }
   ],
   "source": [
    "for it in range(NUM_ITERATIONS):\n",
    "    print(f\"Iteración {it+1}/{NUM_ITERATIONS}\")\n",
    "    updated_P = [row.copy() for row in P]\n",
    "    updated_Q = [row.copy() for row in Q]\n",
    "\n",
    "    # Recorremos sólo las valoraciones reales:\n",
    "    for _, row in train_df.iterrows():\n",
    "        u = int(row.user_id) - 1\n",
    "        i = int(row.book_id) - 1\n",
    "        r_ui = float(row.rating)\n",
    "\n",
    "        # Cálculo del error\n",
    "        pred = sum(P[u][k] * Q[i][k] for k in range(NUM_FACTORS))\n",
    "        e = r_ui - pred\n",
    "\n",
    "        # Actualización por cada factor latente\n",
    "        for k in range(NUM_FACTORS):\n",
    "            p_uk = P[u][k]\n",
    "            q_ik = Q[i][k]\n",
    "            grad_p = e * q_ik - REGULARIZATION * p_uk\n",
    "            grad_q = e * p_uk - REGULARIZATION * q_ik\n",
    "            updated_P[u][k] += LEARNING_RATE * grad_p\n",
    "            updated_Q[i][k] += LEARNING_RATE * grad_q\n",
    "\n",
    "    P, Q = updated_P, updated_Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8314f262",
   "metadata": {},
   "source": [
    "### 6. Evaluación del modelo\n",
    "Para evitar problemas con NaN imputamos la media global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3271820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMF → MAE: 0.7740, RMSE: 0.9903\n"
     ]
    }
   ],
   "source": [
    "def get_regression_metrics(test_df, pred_matrix):\n",
    "    \"\"\"devolvemos MAE y RMSE para todas las valoraciones de test_df\"\"\"\n",
    "    y_true = test_df['rating'].values\n",
    "    y_pred = [\n",
    "        pred_matrix[int(row.user_id)-1, int(row.book_id)-1]\n",
    "        for _, row in test_df.iterrows()\n",
    "    ]\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return mae, rmse\n",
    "\n",
    "pred_matrix = np.dot(np.array(P), np.array(Q).T)\n",
    "pred_matrix = np.clip(pred_matrix, MIN_RATING, MAX_RATING) # usamos clip para asegurar que todos los ratings queden en el intervalo que queremos\n",
    "global_mean = train_df.rating.mean()\n",
    "pred_matrix = np.nan_to_num(pred_matrix, nan=global_mean)\n",
    "\n",
    "mae, rmse = get_regression_metrics(test_df, pred_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32281e40",
   "metadata": {},
   "source": [
    "#### Métricas de clasificación binaria\n",
    "\n",
    "Tanto el **RMSE** como el **MAE** son métricas de regresión (distancia a la realidad de la predicción), mientras que el **F1-score** o el **Recall** miden la precisión con que nuestro modelo recomienda algo que, efectivamente, gusta al usuario. Para ello desarrollamos la siguiente función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f810adc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6901\n",
      "Recall: 1.0000\n",
      "F1: 0.8166\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# redondeamos al entero más próximo (1…5)\n",
    "pred_matrix_int = np.rint(pred_matrix).astype(int)\n",
    "\n",
    "def get_binary_metrics_rounded(test_df, pred_matrix_int, threshold=4):\n",
    "    y_true = (test_df.rating >= threshold).astype(int).values\n",
    "    y_pred = [\n",
    "        1 if pred_matrix_int[int(r.user_id)-1, int(r.book_id)-1] >= threshold else 0\n",
    "        for _, r in test_df.iterrows()\n",
    "    ]\n",
    "    return precision_score(y_true, y_pred, zero_division=0), \\\n",
    "           recall_score   (y_true, y_pred, zero_division=0), \\\n",
    "           f1_score       (y_true, y_pred, zero_division=0)\n",
    "\n",
    "prec, rec, f1 = get_binary_metrics_rounded(test_df, pred_matrix_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456e427c",
   "metadata": {},
   "source": [
    "#### nDCG@K (Normalized Discounted Cumulative Gain)\n",
    "\n",
    "Para evaluar la **calidad del ranking** de nuestras recomendaciones usamos nDCG@K, que mide:\n",
    "\n",
    "1. Relevancia: asignamos a cada ítem una relevancia real (por ejemplo `1` si la valoración ≥4, `0` en otro caso).  \n",
    "2. Posición: los ítems recomendados en los primeros puestos pesan más que los de atrás, mediante un descuento logarítmico.\n",
    "\n",
    "- **1.0** indica ranking perfecto (los K ítems más relevantes están en las primeras posiciones).  \n",
    "- **0.0** indica que ninguno de los K primeros es relevante.\n",
    "\n",
    "En nuestro experimento usamos **nDCG@10** para comparar KNN, PMF y BeMF, comprobando si colocamos primero las recomendaciones que el usuario considera más valiosas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab65c43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG@10 = 0.0456\n"
     ]
    }
   ],
   "source": [
    "def get_user_ndcg(u, pred_matrix, test_df, K=5, rel_col='rel'):\n",
    "    \"\"\"\n",
    "    nDCG@K para el usuario u (0-based).\n",
    "    test_df debe tener columnas ['user_id','book_id', rel_col].\n",
    "    rel_col es la relevancia (binaria o graduada).\n",
    "    \"\"\"\n",
    "    user_ratings = test_df[test_df.user_id == (u+1)]\n",
    "    if user_ratings.empty:\n",
    "        return None\n",
    "\n",
    "    true_rels = {\n",
    "        int(row.book_id)-1: row[rel_col]\n",
    "        for _, row in user_ratings.iterrows()\n",
    "    }\n",
    "\n",
    "    scores = pred_matrix[u]\n",
    "    # Top-K \n",
    "    top_k = np.argsort(scores)[::-1][:K]\n",
    "\n",
    "    dcg = 0.0\n",
    "    for rank, item in enumerate(top_k, start=1):\n",
    "        rel = true_rels.get(item, 0)\n",
    "        dcg += (2**rel - 1) / np.log2(rank + 1)\n",
    "\n",
    "    ideal_rels = sorted(true_rels.values(), reverse=True)[:K]\n",
    "    idcg = sum((2**rel - 1) / np.log2(idx + 1)\n",
    "               for idx, rel in enumerate(ideal_rels, start=1))\n",
    "\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def get_ndcg(pred_matrix, test_df, K=5, rel_col='rel'):\n",
    "    \"\"\"\n",
    "    nDCG@K promedio sobre todos los usuarios con al menos una valoración.\n",
    "    \"\"\"\n",
    "    total, count = 0.0, 0\n",
    "    num_users = pred_matrix.shape[0]\n",
    "    for u in range(num_users):\n",
    "        val = get_user_ndcg(u, pred_matrix, test_df, K, rel_col)\n",
    "        if val is not None:\n",
    "            total += val\n",
    "            count += 1\n",
    "    return (total / count) if count > 0 else 0.0\n",
    "\n",
    "test_df['rel'] = (test_df.rating >= 4).astype(int)\n",
    "\n",
    "\n",
    "# nDCG@10\n",
    "ndcg_10 = get_ndcg(pred_matrix, test_df, K=10, rel_col='rel')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6e5c62",
   "metadata": {},
   "source": [
    "#### Métricas completas y análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d1606c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMF → MAE: 0.7740, RMSE: 0.9903\n",
      "Precision: 0.6901\n",
      "Recall: 1.0000\n",
      "F1: 0.8166\n",
      "nDCG@10 = 0.0456\n"
     ]
    }
   ],
   "source": [
    "print(f\"PMF → MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\\nRecall: {rec:.4f}\\nF1: {f1:.4f}\")\n",
    "print(f\"nDCG@10 = {ndcg_10:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3f9c08",
   "metadata": {},
   "source": [
    "- **MAE = 0.774:** en media, las predicciones de PMF se desvían **menos de 0.8 estrellas** de la valoración real.  \n",
    "- **RMSE = 0.990:** al penalizar más los errores grandes, vemos que el “error típico” queda justo por debajo de **1 estrella**.\n",
    "\n",
    "Estos números indican que PMF capta bien la tendencia general de cada usuario, reduciendo el error absoluto promedio respecto al baseline (≈0.89 MAE). Sin embargo, como revela el nDCG bajo, aún le cuesta ordenar bien los top-K para ranking.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0475ea",
   "metadata": {},
   "source": [
    "- **Recall = 1.0**, no hay falsos negativos.\n",
    "- **Precision = 0.69**, 31% de falsos positivos.\n",
    "- **F1 = 0.82**, media armónica entre ambos, fruto de un modelo que tiende a sobre-predecir \"likes\" para no perder ninguno. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7557fe11",
   "metadata": {},
   "source": [
    "## Incluimos un sesgo\n",
    "\n",
    "Como un 4 no significa lo mismo para 2 usuarios diferentes, incluimos sesgo tanto en los usuarios como en los libros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2799ed7",
   "metadata": {},
   "source": [
    "**SGD iterativo** con sesgos, devolviendo mu y las listas de sesgos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ffa514af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def biased_train(train_df, P, Q, lr, reg_f, reg_b, epochs):\n",
    "    # Convertir a NumPy arrays si vienen como listas\n",
    "    P = np.array(P, dtype=np.float32)\n",
    "    Q = np.array(Q, dtype=np.float32)\n",
    "\n",
    "    # Sesgos\n",
    "    mu  = train_df.rating.mean()\n",
    "    b_u = np.zeros(P.shape[0], dtype=np.float32)\n",
    "    b_i = np.zeros(Q.shape[0], dtype=np.float32)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for _, row in train_df.iterrows():\n",
    "            u = int(row.user_id) - 1\n",
    "            i = int(row.book_id) - 1\n",
    "            r = row.rating\n",
    "\n",
    "            # Aquí ya puedes usar dot\n",
    "            pred = mu + b_u[u] + b_i[i] + P[u].dot(Q[i])\n",
    "            err  = r - pred\n",
    "\n",
    "            # Actualizar sesgos\n",
    "            b_u[u] += lr * (err - reg_b * b_u[u])\n",
    "            b_i[i] += lr * (err - reg_b * b_i[i])\n",
    "\n",
    "            # Actualizar factores\n",
    "            P[u]    += lr * (err * Q[i] - reg_f * P[u])\n",
    "            Q[i]    += lr * (err * P[u] - reg_f * Q[i])\n",
    "\n",
    "    return mu, b_u, b_i, P, Q\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb45afc",
   "metadata": {},
   "source": [
    "#### Matriz de predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "beb48b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def biased_pred_matrix(mu, b_u, b_i, P, Q, min_r=1.0, max_r=5.0):\n",
    "    \"\"\"\n",
    "    Devuelve pred_matrix de forma (NUM_USERS, NUM_ITEMS) con sesgos.\n",
    "    \"\"\"\n",
    "    base = np.dot(P, Q.T)              \n",
    "    preds = mu + b_u[:, None] + b_i[None, :] + base\n",
    "    return np.clip(preds, min_r, max_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846c97e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, b_u, b_i, P, Q = biased_train(\n",
    "    train_df,\n",
    "    P, Q,\n",
    "    lr=0.001,\n",
    "    reg_f=0.1,\n",
    "    reg_b=0.01,\n",
    "    epochs=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245f94f2",
   "metadata": {},
   "source": [
    "#### Métricas para PMF con sesgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f54859",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.98 GiB for an array with shape (53424, 10000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m precision_score, recall_score, f1_score\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m pred_matrix = \u001b[43mbiased_pred_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_u\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m mae, rmse = get_regression_metrics(test_df, pred_matrix)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPMF con sesgo → MAE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmae\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mbiased_pred_matrix\u001b[39m\u001b[34m(mu, b_u, b_i, P, Q, min_r, max_r)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mDevuelve pred_matrix de forma (NUM_USERS, NUM_ITEMS) con sesgos.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m base = np.dot(P, Q.T)              \n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m preds = \u001b[43mmu\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_u\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_i\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.clip(preds, min_r, max_r)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 3.98 GiB for an array with shape (53424, 10000) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "pred_matrix = biased_pred_matrix(mu, b_u, b_i, P, Q, 1.0, 5.0)\n",
    "\n",
    "mae, rmse = get_regression_metrics(test_df, pred_matrix)\n",
    "print(f\"PMF con sesgo → MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "\n",
    "global_mean = train_df.rating.mean()\n",
    "thr = int(np.floor(global_mean))   # ej. si media≈3.8, thr=3\n",
    "\n",
    "pred_int = np.rint(pred_matrix).astype(int)\n",
    "y_true_bin = (test_df.rating.values >= thr).astype(int)\n",
    "y_pred_bin = [\n",
    "    1 if pred_int[int(r.user_id)-1, int(r.book_id)-1] >= thr else 0\n",
    "    for _, r in test_df.iterrows()\n",
    "]\n",
    "\n",
    "prec = precision_score(y_true_bin, y_pred_bin, zero_division=0)\n",
    "rec  = recall_score   (y_true_bin, y_pred_bin, zero_division=0)\n",
    "f1   = f1_score       (y_true_bin, y_pred_bin, zero_division=0)\n",
    "print(f\"Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "# nDCG@10\n",
    "test_df['rel'] = y_true_bin\n",
    "ndcg10 = get_ndcg(pred_matrix, test_df, K=10, rel_col='rel')\n",
    "print(f\"nDCG@10: {ndcg10:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
