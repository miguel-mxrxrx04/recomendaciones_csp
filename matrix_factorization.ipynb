{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5078a3d3",
   "metadata": {},
   "source": [
    "### 1. Importamos librerías y cargamos los datos procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "805b96cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "processed_dir = 'data/processed'\n",
    "train_df = pd.read_csv(os.path.join(processed_dir, 'train.csv'))\n",
    "test_df  = pd.read_csv(os.path.join(processed_dir, 'test.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5523e",
   "metadata": {},
   "source": [
    "### 2. Declaramos nuestros hiperparámetros y constantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24650b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITERATIONS = 10      # iteraciones\n",
    "MIN_RATING, MAX_RATING = 1.0, 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e15281",
   "metadata": {},
   "source": [
    "#### Matriz de valoraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cf6d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista de listas (0 donde no hay rating)\n",
    "num_users = int(max(train_df.user_id.max(), test_df.user_id.max()))\n",
    "num_items = int(max(train_df.book_id.max(), test_df.book_id.max()))\n",
    "R = [[0.0]*num_items for _ in range(num_users)]\n",
    "for _, row in train_df.iterrows():\n",
    "    u = int(row.user_id) - 1\n",
    "    i = int(row.book_id) - 1\n",
    "    R[u][i] = float(row.rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29086b",
   "metadata": {},
   "source": [
    "### 3. Inicializamos el modelo, hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00c529e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FACTORS    = 7       # f\n",
    "LEARNING_RATE  = 0.001   # γ\n",
    "REGULARIZATION = 0.1     # λ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3767dc5",
   "metadata": {},
   "source": [
    "### 4. Creamos P y Q con uniformes en [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d6847f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "P = [[random.random() for _ in range(NUM_FACTORS)] for _ in range(num_users)]\n",
    "Q = [[random.random() for _ in range(NUM_FACTORS)] for _ in range(num_items)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8079dd9d",
   "metadata": {},
   "source": [
    "### 5. SGD iterativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef9d328f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración 1/10\n",
      "Iteración 2/10\n",
      "Iteración 3/10\n",
      "Iteración 4/10\n",
      "Iteración 5/10\n",
      "Iteración 6/10\n",
      "Iteración 7/10\n",
      "Iteración 8/10\n",
      "Iteración 9/10\n",
      "Iteración 10/10\n"
     ]
    }
   ],
   "source": [
    "for it in range(NUM_ITERATIONS):\n",
    "    print(f\"Iteración {it+1}/{NUM_ITERATIONS}\")\n",
    "    updated_P = [row.copy() for row in P]\n",
    "    updated_Q = [row.copy() for row in Q]\n",
    "\n",
    "    # Recorremos sólo las valoraciones reales:\n",
    "    for _, row in train_df.iterrows():\n",
    "        u = int(row.user_id) - 1\n",
    "        i = int(row.book_id) - 1\n",
    "        r_ui = float(row.rating)\n",
    "\n",
    "        # Cálculo del error\n",
    "        pred = sum(P[u][k] * Q[i][k] for k in range(NUM_FACTORS))\n",
    "        e = r_ui - pred\n",
    "\n",
    "        # Actualización por cada factor latente\n",
    "        for k in range(NUM_FACTORS):\n",
    "            p_uk = P[u][k]\n",
    "            q_ik = Q[i][k]\n",
    "            grad_p = e * q_ik - REGULARIZATION * p_uk\n",
    "            grad_q = e * p_uk - REGULARIZATION * q_ik\n",
    "            updated_P[u][k] += LEARNING_RATE * grad_p\n",
    "            updated_Q[i][k] += LEARNING_RATE * grad_q\n",
    "\n",
    "    P, Q = updated_P, updated_Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8314f262",
   "metadata": {},
   "source": [
    "### 6. Evaluación del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3271820",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mae, rmse\n\u001b[32m     12\u001b[39m pred_matrix = np.dot(np.array(P), np.array(Q).T)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m mae, rmse = \u001b[43mget_regression_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_matrix\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mget_regression_metrics\u001b[39m\u001b[34m(test_df, pred_matrix)\u001b[39m\n\u001b[32m      3\u001b[39m y_true = test_df[\u001b[33m'\u001b[39m\u001b[33mrating\u001b[39m\u001b[33m'\u001b[39m].values\n\u001b[32m      4\u001b[39m y_pred = [\n\u001b[32m      5\u001b[39m     pred_matrix[\u001b[38;5;28mint\u001b[39m(row.user_id)-\u001b[32m1\u001b[39m, \u001b[38;5;28mint\u001b[39m(row.book_id)-\u001b[32m1\u001b[39m]\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m test_df.iterrows()\n\u001b[32m      7\u001b[39m ]\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m mae  = \u001b[43mmean_absolute_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m mae, rmse\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MAM\\OneDrive - Universidad Politécnica de Madrid\\Escritorio\\Apuntes_CDIA\\recomendaciones_csp\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MAM\\OneDrive - Universidad Politécnica de Madrid\\Escritorio\\Apuntes_CDIA\\recomendaciones_csp\\venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:277\u001b[39m, in \u001b[36mmean_absolute_error\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Mean absolute error regression loss.\u001b[39;00m\n\u001b[32m    223\u001b[39m \n\u001b[32m    224\u001b[39m \u001b[33;03mRead more in the :ref:`User Guide <mean_absolute_error>`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    272\u001b[39m \u001b[33;03m0.85...\u001b[39;00m\n\u001b[32m    273\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    274\u001b[39m xp, _ = get_namespace(y_true, y_pred, sample_weight, multioutput)\n\u001b[32m    276\u001b[39m _, y_true, y_pred, sample_weight, multioutput = (\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[43m_check_reg_targets_with_floating_dtype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m )\n\u001b[32m    282\u001b[39m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[32m    284\u001b[39m output_errors = _average(\n\u001b[32m    285\u001b[39m     xp.abs(y_pred - y_true), weights=sample_weight, axis=\u001b[32m0\u001b[39m, xp=xp\n\u001b[32m    286\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MAM\\OneDrive - Universidad Politécnica de Madrid\\Escritorio\\Apuntes_CDIA\\recomendaciones_csp\\venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:198\u001b[39m, in \u001b[36m_check_reg_targets_with_floating_dtype\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Ensures that y_true, y_pred, and sample_weight correspond to the same\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[33;03mregression task.\u001b[39;00m\n\u001b[32m    150\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    194\u001b[39m \u001b[33;03m    correct keyword.\u001b[39;00m\n\u001b[32m    195\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    196\u001b[39m dtype_name = _find_matching_floating_dtype(y_true, y_pred, sample_weight, xp=xp)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m y_type, y_true, y_pred, multioutput = \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;66;03m# _check_reg_targets does not accept sample_weight as input.\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[38;5;66;03m# Convert sample_weight's data type separately to match dtype_name.\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MAM\\OneDrive - Universidad Politécnica de Madrid\\Escritorio\\Apuntes_CDIA\\recomendaciones_csp\\venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:106\u001b[39m, in \u001b[36m_check_reg_targets\u001b[39m\u001b[34m(y_true, y_pred, multioutput, dtype, xp)\u001b[39m\n\u001b[32m    104\u001b[39m check_consistent_length(y_true, y_pred)\n\u001b[32m    105\u001b[39m y_true = check_array(y_true, ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=dtype)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m y_pred = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_true.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    109\u001b[39m     y_true = xp.reshape(y_true, (-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MAM\\OneDrive - Universidad Politécnica de Madrid\\Escritorio\\Apuntes_CDIA\\recomendaciones_csp\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1107\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1102\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m expected <= 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1103\u001b[39m         % (array.ndim, estimator_name)\n\u001b[32m   1104\u001b[39m     )\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1107\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1116\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MAM\\OneDrive - Universidad Politécnica de Madrid\\Escritorio\\Apuntes_CDIA\\recomendaciones_csp\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MAM\\OneDrive - Universidad Politécnica de Madrid\\Escritorio\\Apuntes_CDIA\\recomendaciones_csp\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "def get_regression_metrics(test_df, pred_matrix):\n",
    "    \"\"\"devolvemos MAE y RMSE para todas las valoraciones de test_df\"\"\"\n",
    "    y_true = test_df['rating'].values\n",
    "    y_pred = [\n",
    "        pred_matrix[int(row.user_id)-1, int(row.book_id)-1]\n",
    "        for _, row in test_df.iterrows()\n",
    "    ]\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return mae, rmse\n",
    "\n",
    "pred_matrix = np.dot(np.array(P), np.array(Q).T)\n",
    "mae, rmse = get_regression_metrics(test_df, pred_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32281e40",
   "metadata": {},
   "source": [
    "#### Métricas de clasificación binaria\n",
    "\n",
    "Tanto el **RMSE** como el **MAE** son métricas de regresión (distancia a la realidad de la predicción), mientras que el **F1-score** o el **Recall** miden la precisión con que nuestro modelo recomienda algo que, efectivamente, gusta al usuario. Para ello desarrollamos la siguiente función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f810adc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def get_binary_metrics(test_df, pred_matrix, threshold=4.0):\n",
    "    \"\"\"\n",
    "    Binariza la tarea en like>=threshold vs dislike<threshold.\n",
    "    Devuelve precision, recall, f1.\n",
    "    \"\"\"\n",
    "    y_true = (test_df['rating'] >= threshold).astype(int).values\n",
    "    y_pred = [\n",
    "        1 if pred_matrix[int(row.user_id)-1, int(row.book_id)-1] >= threshold else 0\n",
    "        for _, row in test_df.iterrows()\n",
    "    ]\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec  = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1   = f1_score(y_true, y_pred, zero_division=0)\n",
    "    return prec, rec, f1\n",
    "\n",
    "prec, rec, f1 = get_binary_metrics(test_df, pred_matrix, threshold = 3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456e427c",
   "metadata": {},
   "source": [
    "#### nDCG@K (Normalized Discounted Cumulative Gain)\n",
    "\n",
    "Para evaluar la **calidad del ranking** de nuestras recomendaciones usamos nDCG@K, que mide:\n",
    "\n",
    "1. **Relevancia**: asignamos a cada ítem una relevancia real (por ejemplo `1` si la valoración ≥4, `0` en otro caso).  \n",
    "2. **Posición**: los ítems recomendados en los primeros puestos pesan más que los de atrás, mediante un descuento logarítmico.\n",
    "\n",
    "- **1.0** indica ranking perfecto (los K ítems más relevantes están en las primeras posiciones).  \n",
    "- **0.0** indica que ninguno de los K primeros es relevante.\n",
    "\n",
    "En nuestro experimento usamos **nDCG@5** para comparar KNN, PMF y SVD, asegurando no solo que predecimos bien los ratings (RMSE/MAE), sino también que colocamos primero las recomendaciones que el usuario considera más valiosas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab65c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_ndcg(u, pred_matrix, test_df, K=5, rel_col='rel'):\n",
    "    \"\"\"\n",
    "    nDCG@K para el usuario u (0-based).\n",
    "    test_df debe tener columnas ['user_id','book_id', rel_col].\n",
    "    rel_col es la relevancia (binaria o graduada).\n",
    "    \"\"\"\n",
    "    user_ratings = test_df[test_df.user_id == (u+1)]\n",
    "    if user_ratings.empty:\n",
    "        return None\n",
    "\n",
    "    true_rels = {\n",
    "        int(row.book_id)-1: row[rel_col]\n",
    "        for _, row in user_ratings.iterrows()\n",
    "    }\n",
    "\n",
    "    scores = pred_matrix[u]\n",
    "    # Top-K \n",
    "    top_k = np.argsort(scores)[::-1][:K]\n",
    "\n",
    "    dcg = 0.0\n",
    "    for rank, item in enumerate(top_k, start=1):\n",
    "        rel = true_rels.get(item, 0)\n",
    "        dcg += (2**rel - 1) / np.log2(rank + 1)\n",
    "\n",
    "    ideal_rels = sorted(true_rels.values(), reverse=True)[:K]\n",
    "    idcg = sum((2**rel - 1) / np.log2(idx + 1)\n",
    "               for idx, rel in enumerate(ideal_rels, start=1))\n",
    "\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def get_ndcg(pred_matrix, test_df, K=5, rel_col='rel'):\n",
    "    \"\"\"\n",
    "    nDCG@K promedio sobre todos los usuarios con al menos una valoración.\n",
    "    \"\"\"\n",
    "    total, count = 0.0, 0\n",
    "    num_users = pred_matrix.shape[0]\n",
    "    for u in range(num_users):\n",
    "        val = get_user_ndcg(u, pred_matrix, test_df, K, rel_col)\n",
    "        if val is not None:\n",
    "            total += val\n",
    "            count += 1\n",
    "    return (total / count) if count > 0 else 0.0\n",
    "\n",
    "test_df['rel'] = (test_df.rating >= 4).astype(int)\n",
    "\n",
    "\n",
    "# nDCG@5\n",
    "ndcg_5 = get_ndcg(pred_matrix, test_df, K=5, rel_col='rel')\n",
    "print(f\"nDCG@5 = {ndcg_5:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7557fe11",
   "metadata": {},
   "source": [
    "## Incluimos un sesgo\n",
    "\n",
    "Como un 4 no significa lo mismo para 2 usuarios diferentes, incluimos sesgo tanto en los usuarios como en los libros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2799ed7",
   "metadata": {},
   "source": [
    "**SGD iterativo** con sesgos, devolviendo mu y las listas de sesgos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa514af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def biased_train(train_df, P, Q, lr, reg_f, reg_b, epochs):\n",
    "    \"\"\"\n",
    "    P, Q y sesgos mu, b_u, b_i usando SGD.\n",
    "    Devuelve mu, b_u, b_i, P, Q actualizados.\n",
    "    \"\"\"\n",
    "    # dimensiones\n",
    "    num_users = int(train_df.user_id.max())\n",
    "    num_items = int(train_df.book_id.max())\n",
    "    \n",
    "    mu  = train_df.rating.mean()\n",
    "    b_u = np.zeros(num_users)\n",
    "    b_i = np.zeros(num_items)\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for _, row in train_df.iterrows():\n",
    "            u = int(row.user_id) - 1\n",
    "            i = int(row.book_id) - 1\n",
    "            r = row.rating\n",
    "            \n",
    "            pred = mu + b_u[u] + b_i[i] + P[u].dot(Q[i])\n",
    "            err  = r - pred\n",
    "            \n",
    "            # actualizar sesgos\n",
    "            b_u[u] += lr * (err - reg_b * b_u[u])\n",
    "            b_i[i] += lr * (err - reg_b * b_i[i])\n",
    "            \n",
    "            # actualizar factores\n",
    "            for k in range(P.shape[1]):\n",
    "                p_uk = P[u, k]\n",
    "                q_ik = Q[i, k]\n",
    "                P[u, k] += lr * (err * q_ik - reg_f * p_uk)\n",
    "                Q[i, k] += lr * (err * p_uk - reg_f * q_ik)\n",
    "    \n",
    "    return mu, b_u, b_i, P, Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb45afc",
   "metadata": {},
   "source": [
    "#### Matriz de predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb48b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def biased_pred_matrix(mu, b_u, b_i, P, Q, min_r=1.0, max_r=5.0):\n",
    "    \"\"\"\n",
    "    Devuelve pred_matrix de forma (NUM_USERS, NUM_ITEMS) con sesgos.\n",
    "    \"\"\"\n",
    "    base = P.dot(Q.T)                 # (NUM_USERS, NUM_ITEMS)\n",
    "    preds = mu + b_u[:, None] + b_i[None, :] + base\n",
    "    return np.clip(preds, min_r, max_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846c97e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, b_u, b_i, P, Q = biased_train(\n",
    "    train_df,\n",
    "    P, Q,\n",
    "    lr=0.001,\n",
    "    reg_f=0.1,\n",
    "    reg_b=0.01,\n",
    "    epochs=20\n",
    ")\n",
    "\n",
    "pred_matrix = biased_pred_matrix(mu, b_u, b_i, P, Q, 1.0, 5.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
