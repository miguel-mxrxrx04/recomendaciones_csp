{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5078a3d3",
   "metadata": {},
   "source": [
    "### 1. Importamos librerías y cargamos los datos procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "805b96cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "processed_dir = 'data/processed'\n",
    "train_df = pd.read_csv(os.path.join(processed_dir, 'train.csv'))\n",
    "test_df  = pd.read_csv(os.path.join(processed_dir, 'test.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5523e",
   "metadata": {},
   "source": [
    "### 2. Declaramos nuestros hiperparámetros y constantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24650b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITERATIONS = 10      # iteraciones\n",
    "MIN_RATING, MAX_RATING = 1.0, 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e15281",
   "metadata": {},
   "source": [
    "#### Matriz de valoraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cf6d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista de listas (0 donde no hay rating)\n",
    "num_users = int(max(train_df.user_id.max(), test_df.user_id.max()))\n",
    "num_items = int(max(train_df.book_id.max(), test_df.book_id.max()))\n",
    "R = [[0.0]*num_items for _ in range(num_users)]\n",
    "for _, row in train_df.iterrows():\n",
    "    u = int(row.user_id) - 1\n",
    "    i = int(row.book_id) - 1\n",
    "    R[u][i] = float(row.rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29086b",
   "metadata": {},
   "source": [
    "### 3. Inicializamos el modelo, hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00c529e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FACTORS    = 7       # f\n",
    "LEARNING_RATE  = 0.001   # γ\n",
    "REGULARIZATION = 0.1     # λ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3767dc5",
   "metadata": {},
   "source": [
    "### 4. Creamos P y Q con uniformes en [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d6847f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "P = [[random.random() for _ in range(NUM_FACTORS)] for _ in range(num_users)]\n",
    "Q = [[random.random() for _ in range(NUM_FACTORS)] for _ in range(num_items)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8079dd9d",
   "metadata": {},
   "source": [
    "### 5. SGD iterativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9d328f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración 1/10\n",
      "Iteración 2/10\n",
      "Iteración 3/10\n",
      "Iteración 4/10\n",
      "Iteración 5/10\n",
      "Iteración 6/10\n",
      "Iteración 7/10\n"
     ]
    }
   ],
   "source": [
    "for it in range(NUM_ITERATIONS):\n",
    "    print(f\"Iteración {it+1}/{NUM_ITERATIONS}\")\n",
    "    updated_P = [row.copy() for row in P]\n",
    "    updated_Q = [row.copy() for row in Q]\n",
    "\n",
    "    # Recorremos sólo las valoraciones reales:\n",
    "    for _, row in train_df.iterrows():\n",
    "        u = int(row.user_id) - 1\n",
    "        i = int(row.book_id) - 1\n",
    "        r_ui = float(row.rating)\n",
    "\n",
    "        # Cálculo del error\n",
    "        pred = sum(P[u][k] * Q[i][k] for k in range(NUM_FACTORS))\n",
    "        e = r_ui - pred\n",
    "\n",
    "        # Actualización por cada factor latente\n",
    "        for k in range(NUM_FACTORS):\n",
    "            p_uk = P[u][k]\n",
    "            q_ik = Q[i][k]\n",
    "            grad_p = e * q_ik - REGULARIZATION * p_uk\n",
    "            grad_q = e * p_uk - REGULARIZATION * q_ik\n",
    "            updated_P[u][k] += LEARNING_RATE * grad_p\n",
    "            updated_Q[i][k] += LEARNING_RATE * grad_q\n",
    "\n",
    "    P, Q = updated_P, updated_Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8314f262",
   "metadata": {},
   "source": [
    "### 6. Evaluación del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3271820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_metrics(test_df, pred_matrix):\n",
    "    \"\"\"devolvemos MAE y RMSE para todas las valoraciones de test_df\"\"\"\n",
    "    y_true = test_df['rating'].values\n",
    "    y_pred = [\n",
    "        pred_matrix[int(row.user_id)-1, int(row.book_id)-1]\n",
    "        for _, row in test_df.iterrows()\n",
    "    ]\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return mae, rmse\n",
    "\n",
    "pred_matrix = np.dot(np.array(P), np.array(Q).T)\n",
    "mae, rmse = get_regression_metrics(test_df, pred_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32281e40",
   "metadata": {},
   "source": [
    "#### Métricas de clasificación binaria\n",
    "\n",
    "Tanto el **RMSE** como el **MAE** son métricas de regresión (distancia a la realidad de la predicción), mientras que el **F1-score** o el **Recall** miden la precisión con que nuestro modelo recomienda algo que, efectivamente, gusta al usuario. Para ello desarrollamos la siguiente función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f810adc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def get_binary_metrics(test_df, pred_matrix, threshold=4.0):\n",
    "    \"\"\"\n",
    "    Binariza la tarea en like>=threshold vs dislike<threshold.\n",
    "    Devuelve precision, recall, f1.\n",
    "    \"\"\"\n",
    "    y_true = (test_df['rating'] >= threshold).astype(int).values\n",
    "    y_pred = [\n",
    "        1 if pred_matrix[int(row.user_id)-1, int(row.book_id)-1] >= threshold else 0\n",
    "        for _, row in test_df.iterrows()\n",
    "    ]\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec  = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1   = f1_score(y_true, y_pred, zero_division=0)\n",
    "    return prec, rec, f1\n",
    "\n",
    "prec, rec, f1 = get_binary_metrics(test_df, pred_matrix, threshold = 3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456e427c",
   "metadata": {},
   "source": [
    "#### nDCG@K (Normalized Discounted Cumulative Gain)\n",
    "\n",
    "Para evaluar la **calidad del ranking** de nuestras recomendaciones usamos nDCG@K, que mide:\n",
    "\n",
    "1. **Relevancia**: asignamos a cada ítem una relevancia real (por ejemplo `1` si la valoración ≥4, `0` en otro caso).  \n",
    "2. **Posición**: los ítems recomendados en los primeros puestos pesan más que los de atrás, mediante un descuento logarítmico.\n",
    "\n",
    "- **1.0** indica ranking perfecto (los K ítems más relevantes están en las primeras posiciones).  \n",
    "- **0.0** indica que ninguno de los K primeros es relevante.\n",
    "\n",
    "En nuestro experimento usamos **nDCG@5** para comparar KNN, PMF y SVD, asegurando no solo que predecimos bien los ratings (RMSE/MAE), sino también que colocamos primero las recomendaciones que el usuario considera más valiosas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab65c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_ndcg(u, pred_matrix, test_df, K=5, rel_col='rel'):\n",
    "    \"\"\"\n",
    "    nDCG@K para el usuario u (0-based).\n",
    "    test_df debe tener columnas ['user_id','book_id', rel_col].\n",
    "    rel_col es la relevancia (binaria o graduada).\n",
    "    \"\"\"\n",
    "    user_ratings = test_df[test_df.user_id == (u+1)]\n",
    "    if user_ratings.empty:\n",
    "        return None\n",
    "\n",
    "    true_rels = {\n",
    "        int(row.book_id)-1: row[rel_col]\n",
    "        for _, row in user_ratings.iterrows()\n",
    "    }\n",
    "\n",
    "    scores = pred_matrix[u]\n",
    "    # Top-K \n",
    "    top_k = np.argsort(scores)[::-1][:K]\n",
    "\n",
    "    dcg = 0.0\n",
    "    for rank, item in enumerate(top_k, start=1):\n",
    "        rel = true_rels.get(item, 0)\n",
    "        dcg += (2**rel - 1) / np.log2(rank + 1)\n",
    "\n",
    "    ideal_rels = sorted(true_rels.values(), reverse=True)[:K]\n",
    "    idcg = sum((2**rel - 1) / np.log2(idx + 1)\n",
    "               for idx, rel in enumerate(ideal_rels, start=1))\n",
    "\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def get_ndcg(pred_matrix, test_df, K=5, rel_col='rel'):\n",
    "    \"\"\"\n",
    "    nDCG@K promedio sobre todos los usuarios con al menos una valoración.\n",
    "    \"\"\"\n",
    "    total, count = 0.0, 0\n",
    "    num_users = pred_matrix.shape[0]\n",
    "    for u in range(num_users):\n",
    "        val = get_user_ndcg(u, pred_matrix, test_df, K, rel_col)\n",
    "        if val is not None:\n",
    "            total += val\n",
    "            count += 1\n",
    "    return (total / count) if count > 0 else 0.0\n",
    "\n",
    "test_df['rel'] = (test_df.rating >= 4).astype(int)\n",
    "\n",
    "\n",
    "# nDCG@5\n",
    "ndcg_5 = get_ndcg(pred_matrix, test_df, K=5, rel_col='rel')\n",
    "print(f\"nDCG@5 = {ndcg_5:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7557fe11",
   "metadata": {},
   "source": [
    "## Incluimos un sesgo\n",
    "\n",
    "Como un 4 no significa lo mismo para 2 usuarios diferentes, incluimos sesgo tanto en los usuarios como en los libros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2799ed7",
   "metadata": {},
   "source": [
    "**SGD iterativo** con sesgos, devolviendo mu y las listas de sesgos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa514af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def biased_train(train_df, P, Q, lr, reg_f, reg_b, epochs):\n",
    "    \"\"\"\n",
    "    P, Q y sesgos mu, b_u, b_i usando SGD.\n",
    "    Devuelve mu, b_u, b_i, P, Q actualizados.\n",
    "    \"\"\"\n",
    "    # dimensiones\n",
    "    num_users = int(train_df.user_id.max())\n",
    "    num_items = int(train_df.book_id.max())\n",
    "    \n",
    "    mu  = train_df.rating.mean()\n",
    "    b_u = np.zeros(num_users)\n",
    "    b_i = np.zeros(num_items)\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for _, row in train_df.iterrows():\n",
    "            u = int(row.user_id) - 1\n",
    "            i = int(row.book_id) - 1\n",
    "            r = row.rating\n",
    "            \n",
    "            pred = mu + b_u[u] + b_i[i] + P[u].dot(Q[i])\n",
    "            err  = r - pred\n",
    "            \n",
    "            # actualizar sesgos\n",
    "            b_u[u] += lr * (err - reg_b * b_u[u])\n",
    "            b_i[i] += lr * (err - reg_b * b_i[i])\n",
    "            \n",
    "            # actualizar factores\n",
    "            for k in range(P.shape[1]):\n",
    "                p_uk = P[u, k]\n",
    "                q_ik = Q[i, k]\n",
    "                P[u, k] += lr * (err * q_ik - reg_f * p_uk)\n",
    "                Q[i, k] += lr * (err * p_uk - reg_f * q_ik)\n",
    "    \n",
    "    return mu, b_u, b_i, P, Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb45afc",
   "metadata": {},
   "source": [
    "#### Matriz de predicciones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
